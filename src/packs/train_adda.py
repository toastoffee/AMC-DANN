import torch
from torch import nn, optim
import warnings
import torch.nn.functional as F
from train.device_utils import get_device
from model.adda import ADDA
from sklearn.metrics import accuracy_score

warnings.filterwarnings('ignore')


def train_adda(
        source_loader,
        target_loader,
        num_epochs: int = 50,
        lr: float = 1e-3,
        device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
):
    """
    ADDAè®­ç»ƒå‡½æ•° - éµå¾ªDANä»£ç é£æ ¼
    """
    # åˆå§‹åŒ–æ¨¡å‹
    model = ADDA(num_classes=11).to(device)

    # é˜¶æ®µ1ï¼šæºåŸŸé¢„è®­ç»ƒä¼˜åŒ–å™¨
    source_optimizer = optim.Adam(
        list(model.source_encoder.parameters()) + list(model.classifier.parameters()),
        lr=lr, weight_decay=1e-4
    )

    # é˜¶æ®µ2ï¼šå¯¹æŠ—è®­ç»ƒä¼˜åŒ–å™¨
    target_optimizer = optim.Adam(model.target_encoder.parameters(), lr=lr / 10, weight_decay=1e-4)
    disc_optimizer = optim.Adam(model.discriminator.parameters(), lr=lr / 10, weight_decay=1e-4)

    # æŸå¤±å‡½æ•°
    ce_loss = nn.CrossEntropyLoss()
    bce_loss = nn.BCEWithLogitsLoss()

    print("=== é˜¶æ®µ1: æºåŸŸç¼–ç å™¨é¢„è®­ç»ƒ ===")

    # é˜¶æ®µ1ï¼šæºåŸŸé¢„è®­ç»ƒ
    model.train()
    for epoch in range(num_epochs // 2):  # ä¸€åŠepochç”¨äºæºé¢„è®­ç»ƒ
        total_cls_loss = 0.0
        batch_count = 0

        for i, (src_data, src_labels, _) in enumerate(source_loader):
            src_data, src_labels = src_data.to(device), src_labels.to(device)

            # æºåŸŸå‰å‘ä¼ æ’­
            logits_src = model(src_data, domain='source')
            cls_loss = ce_loss(logits_src, src_labels)

            # åå‘ä¼ æ’­
            source_optimizer.zero_grad()
            cls_loss.backward()
            source_optimizer.step()

            total_cls_loss += cls_loss.item()
            batch_count += 1

            if i % 20 == 0:
                print(f"é¢„è®­ç»ƒ Epoch [{epoch + 1}/{num_epochs // 2}] [Batch {i}/{len(source_loader)}] | "
                      f"Cls Loss: {cls_loss.item():.4f}")

        # æ¯5ä¸ªepochéªŒè¯ä¸€æ¬¡
        if epoch % 5 == 0:
            source_acc = validate_model(model, source_loader, device, domain='source')
            print(f"é¢„è®­ç»ƒ Epoch [{epoch + 1}/{num_epochs // 2}] | "
                  f"å¹³å‡Cls Loss: {total_cls_loss / batch_count:.4f} | "
                  f"æºåŸŸå‡†ç¡®ç‡: {source_acc:.2f}%")

    print("=== é˜¶æ®µ2: å¯¹æŠ—è®­ç»ƒ ===")

    # è§£å†»ç›®æ ‡ç¼–ç å™¨
    for param in model.target_encoder.parameters():
        param.requires_grad = True

    # è·å–è¾ƒçŸ­çš„è¿­ä»£æ¬¡æ•°
    min_len = min(len(source_loader), len(target_loader))

    # é˜¶æ®µ2ï¼šå¯¹æŠ—è®­ç»ƒ
    for epoch in range(num_epochs // 2, num_epochs):
        total_disc_loss = 0.0
        total_target_loss = 0.0
        batch_count = 0

        src_iter = iter(source_loader)
        tgt_iter = iter(target_loader)

        for i in range(min_len):
            # è·å–æ•°æ®
            src_data, src_labels, _ = next(src_iter)
            tgt_data, _, _ = next(tgt_iter)

            min_batch = min(src_data.size(0), tgt_data.size(0))
            src_data = src_data[:min_batch].to(device)
            tgt_data = tgt_data[:min_batch].to(device)

            # ===== è®­ç»ƒåˆ¤åˆ«å™¨ =====
            model.target_encoder.eval()
            model.discriminator.train()

            # æºåŸŸç‰¹å¾ï¼ˆå›ºå®šï¼‰
            with torch.no_grad():
                _, feat_src = model(src_data, domain='source', return_features=True)

            # ç›®æ ‡åŸŸç‰¹å¾
            _, feat_tgt = model(tgt_data, domain='target', return_features=True)

            # åˆ¤åˆ«å™¨é¢„æµ‹
            pred_src = model.get_domain_prediction(feat_src.detach())
            pred_tgt = model.get_domain_prediction(feat_tgt.detach())

            # åˆ¤åˆ«å™¨æŸå¤±
            loss_disc_src = bce_loss(pred_src, torch.ones_like(pred_src))
            loss_disc_tgt = bce_loss(pred_tgt, torch.zeros_like(pred_tgt))
            loss_disc = 1.0 * (loss_disc_src + loss_disc_tgt) / 2

            disc_optimizer.zero_grad()
            loss_disc.backward()
            disc_optimizer.step()

            # ===== è®­ç»ƒç›®æ ‡ç¼–ç å™¨ =====
            model.target_encoder.train()
            model.discriminator.eval()

            # ç›®æ ‡åŸŸç‰¹å¾
            _, feat_tgt = model(tgt_data, domain='target', return_features=True)
            pred_tgt = model.get_domain_prediction(feat_tgt)

            # ç›®æ ‡ç¼–ç å™¨æŸå¤±ï¼ˆå€’ç½®æ ‡ç­¾ï¼‰
            loss_target = 1.0 * bce_loss(pred_tgt, torch.ones_like(pred_tgt))

            target_optimizer.zero_grad()
            loss_target.backward()
            target_optimizer.step()

            total_disc_loss += loss_disc.item()
            total_target_loss += loss_target.item()
            batch_count += 1

            if i % 20 == 0:
                print(f"å¯¹æŠ—è®­ç»ƒ Epoch [{epoch + 1}/{num_epochs}] [Batch {i}/{min_len}] | "
                      f"Disc Loss: {loss_disc.item():.4f} | Target Loss: {loss_target.item():.4f}")

        # æ¯5ä¸ªepochéªŒè¯ä¸€æ¬¡
        if epoch % 5 == 0:
            target_acc = validate_model(model, target_loader, device, domain='target')
            print(f"å¯¹æŠ—è®­ç»ƒ Epoch [{epoch + 1}/{num_epochs}] | "
                  f"å¹³å‡Disc Loss: {total_disc_loss / batch_count:.4f} | "
                  f"å¹³å‡Target Loss: {total_target_loss / batch_count:.4f} | "
                  f"ç›®æ ‡åŸŸå‡†ç¡®ç‡: {target_acc:.2f}%")

    return model


def validate_model(model, valid_loader, device, domain: str = 'target'):
    """
    ğŸ¯ éªŒè¯æ¨¡å‹æ€§èƒ½ - é€‚é…ADDAçš„åŒç¼–ç å™¨ç»“æ„
    """
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for data, labels, snr in valid_loader:
            data = data.to(device, dtype=torch.float32)
            labels = labels.to(device)

            # æ ¹æ®åŸŸé€‰æ‹©ç¼–ç å™¨
            logits = model(data, domain=domain)
            _, predicted = torch.max(logits.data, 1)

            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼

    return accuracy


if __name__ == "__main__":
    from dataset.dataloader_helper import DataloaderHelper

    device: torch.device = get_device()

    batch_size = 1024
    num_epochs = 50

    # åŠ è½½æ•°æ®
    source_train_loader, _ = DataloaderHelper.dataloader_10a(batch_size, 1.0)
    target_train_loader, _ = DataloaderHelper.dataloader_22(batch_size, 1.0)

    # è®­ç»ƒADDAæ¨¡å‹
    trained_model = train_adda(
        source_train_loader,
        target_train_loader,
        num_epochs=num_epochs,
        device=device
    )

    # æœ€ç»ˆéªŒè¯
    final_acc = validate_model(trained_model, target_train_loader, device, domain='target')
    print(f"ğŸ¯ ADDAè®­ç»ƒå®Œæˆï¼æœ€ç»ˆç›®æ ‡åŸŸå‡†ç¡®ç‡: {final_acc:.2f}%")