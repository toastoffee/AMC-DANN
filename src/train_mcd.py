import torch
from torch import nn, optim
import warnings


import torch.nn.functional as F
from train.device_utils import get_device
from dataset.dataloader_helper import DataloaderHelper
from model.mcd import MCD
from model import modelutils
from sklearn.metrics import accuracy_score, top_k_accuracy_score

warnings.filterwarnings('ignore')


def discrepancy(out1, out2):
    return torch.mean(torch.abs(F.softmax(out1) - F.softmax(out2)))


def run_train():
    device: torch.device = get_device()

    batch_size = 512
    num_epochs = 50
    num_k = 4

    source_train_loader, _ = DataloaderHelper.dataloader_10a(batch_size, 1.0)
    target_train_loader, _ = DataloaderHelper.dataloader_22(batch_size, 1.0)

    model = MCD()
    model.to(device)
    model.train()

    opt_g = optim.Adam(model.generator.parameters(), lr=1e-3, weight_decay=5e-4)
    opt_c1 = optim.Adam(model.classifier1.parameters(), lr=1e-3, weight_decay=5e-4)
    opt_c2 = optim.Adam(model.classifier2.parameters(), lr=1e-3, weight_decay=5e-4)

    def reset_grad():
        opt_g.zero_grad()
        opt_c1.zero_grad()
        opt_c2.zero_grad()

    criterion = nn.CrossEntropyLoss().to(device)

    min_len = min(len(source_train_loader), len(target_train_loader))

    for epoch in range(num_epochs):

        combined_loader = zip(iter(source_train_loader), iter(target_train_loader))
        for batch_idx, ((source_data, source_labels, source_snr),
                        (target_data, target_labels, target_snr)) in enumerate(combined_loader):

            if batch_idx >= min_len:
                break

            data_s = source_data.to(device, dtype=torch.float32)
            label_s = source_labels.to(device)
            data_t = target_data.to(device, dtype=torch.float32)
            data = torch.cat([data_s, data_t], dim=0)

            # step 1: train G and C, minimizing classification loss
            out_s1, out_s2 = model(data_s)
            loss_s1 = criterion(out_s1, label_s)
            loss_s2 = criterion(out_s2, label_s)
            loss_s = loss_s1 + loss_s2

            reset_grad()
            loss_s.backward()
            opt_g.step()
            opt_c1.step()
            opt_c2.step()

            # step 2: freeze G, train classifier
            out_s1, out_s2 = model(data_s)
            out_t1, out_t2 = model(data_t)
            loss_s1 = criterion(out_s1, label_s)
            loss_s2 = criterion(out_s2, label_s)
            loss_s = loss_s1 + loss_s2
            loss_dis = discrepancy(out_t1, out_t2)
            loss = loss_s - loss_dis
            reset_grad()
            loss.backward()
            opt_c1.step()
            opt_c2.step()

            # step 3: train G
            for i in range(4):
                out_t1, out_t2 = model(data_t)
                loss_dis = discrepancy(out_t1, out_t2)
                reset_grad()
                loss_dis.backward()
                opt_g.step()

            if batch_idx % 50 == 0:
                print(f'[Step2]Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{min_len}], '
                      f'Loss s1: {loss_s1.item():.4f}, '
                      f'Loss s2: {loss_s2.item():.4f}, '
                      f'Discrepancy: {loss_dis.item():.4f}, ')

        if epoch % 5 == 0:
            validate_model(model, target_train_loader, device)


def validate_model(model, valid_loader, device):
    """
    üéØ È™åËØÅÊ®°ÂûãÂú®Ê∫êÂüü‰∏äÁöÑÊÄßËÉΩ
    """
    model.eval()
    correct1 = 0
    correct2 = 0
    total = 0

    with torch.no_grad():
        for data, labels, snr in valid_loader:
            data = data.to(device, dtype=torch.float32)
            labels = labels.to(device)

            class_logits1, class_logits2 = model(data)
            _, predicted1 = torch.max(class_logits1.data, 1)
            _, predicted2 = torch.max(class_logits2.data, 1)

            total += labels.size(0)
            correct1 += (predicted1 == labels).sum().item()
            correct2 += (predicted2 == labels).sum().item()

    accuracy1 = 100 * correct1 / total
    accuracy2 = 100 * correct2 / total

    print(f"acc1:{accuracy1}, acc2:{accuracy2}")

    model.train()  # ÊÅ¢Â§çËÆ≠ÁªÉÊ®°Âºè

    return 0


if __name__ == "__main__":
    run_train()
